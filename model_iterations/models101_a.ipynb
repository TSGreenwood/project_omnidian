{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification models featuring tilt, azimuth, lat, long, module count, inverter count, and other features in lowercase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/humanperson/anaconda3/envs/learn2env/lib/python3.7/site-packages/sklearn/externals/six.py:31: DeprecationWarning: The module is deprecated in version 0.21 and will be removed in version 0.23 since we've dropped support for Python 2.7. Please rely on the official version of six (https://pypi.org/project/six/).\n",
      "  \"(https://pypi.org/project/six/).\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier, BaggingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer, make_column_transformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn import tree\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.utils import resample\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn-paper')\n",
    "import pickle\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df101_a = pd.read_csv('data/eda101_a.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df101_a.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 710 entries, 0 to 709\n",
      "Data columns (total 15 columns):\n",
      "ticket_id                         710 non-null int64\n",
      "asset_id                          710 non-null int64\n",
      "root_cause                        710 non-null object\n",
      "ticket_creation_reason            710 non-null object\n",
      "latitude                          710 non-null float64\n",
      "longitude                         710 non-null float64\n",
      "tilt                              710 non-null float64\n",
      "azimuth                           710 non-null float64\n",
      "inverter_count                    710 non-null int64\n",
      "module_count                      710 non-null int64\n",
      "ticket_origin                     710 non-null object\n",
      "service_partner                   710 non-null object\n",
      "date_ticket_initially_assigned    710 non-null object\n",
      "installed_by                      710 non-null object\n",
      "installation_date                 710 non-null object\n",
      "dtypes: float64(4), int64(4), object(7)\n",
      "memory usage: 83.3+ KB\n"
     ]
    }
   ],
   "source": [
    "df101_a.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That didn't work, lets grab all the column names containing 'date' and convert them to datetime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['date_ticket_initially_assigned', 'installation_date']\n"
     ]
    }
   ],
   "source": [
    "dates = [col for col in df101_a.columns if 'date' in col]\n",
    "print(dates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import to_datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df101_a[dates] = df101_a[dates].apply(pd.to_datetime, format='%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 710 entries, 0 to 709\n",
      "Data columns (total 15 columns):\n",
      "ticket_id                         710 non-null int64\n",
      "asset_id                          710 non-null int64\n",
      "root_cause                        710 non-null object\n",
      "ticket_creation_reason            710 non-null object\n",
      "latitude                          710 non-null float64\n",
      "longitude                         710 non-null float64\n",
      "tilt                              710 non-null float64\n",
      "azimuth                           710 non-null float64\n",
      "inverter_count                    710 non-null int64\n",
      "module_count                      710 non-null int64\n",
      "ticket_origin                     710 non-null object\n",
      "service_partner                   710 non-null object\n",
      "date_ticket_initially_assigned    710 non-null datetime64[ns]\n",
      "installed_by                      710 non-null object\n",
      "installation_date                 710 non-null datetime64[ns]\n",
      "dtypes: datetime64[ns](2), float64(4), int64(4), object(5)\n",
      "memory usage: 83.3+ KB\n"
     ]
    }
   ],
   "source": [
    "df101_a.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's convert be certain our ticket_id and asset_id are objects for encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 710 entries, 0 to 709\n",
      "Data columns (total 15 columns):\n",
      "ticket_id                         710 non-null object\n",
      "asset_id                          710 non-null object\n",
      "root_cause                        710 non-null object\n",
      "ticket_creation_reason            710 non-null object\n",
      "latitude                          710 non-null float64\n",
      "longitude                         710 non-null float64\n",
      "tilt                              710 non-null float64\n",
      "azimuth                           710 non-null float64\n",
      "inverter_count                    710 non-null int64\n",
      "module_count                      710 non-null int64\n",
      "ticket_origin                     710 non-null object\n",
      "service_partner                   710 non-null object\n",
      "date_ticket_initially_assigned    710 non-null datetime64[ns]\n",
      "installed_by                      710 non-null object\n",
      "installation_date                 710 non-null datetime64[ns]\n",
      "dtypes: datetime64[ns](2), float64(4), int64(2), object(7)\n",
      "memory usage: 83.3+ KB\n"
     ]
    }
   ],
   "source": [
    "df101_a[['ticket_id', 'asset_id']] = df101_a[['ticket_id', 'asset_id']].astype('object')\n",
    "df101_a.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_origin_omnidian_customer = df101_a.loc[df101_a.ticket_origin =='origin_omnidian_customer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "root_cause_normal_wear_and_tear                    170\n",
       "root_cause_major_component_failure_warranty         54\n",
       "root_cause_non-service_support                      19\n",
       "root_cause_homeowner                                10\n",
       "root_cause_installer_workmanship                    10\n",
       "root_cause_environmental                             5\n",
       "root_cause_roof_issue                                4\n",
       "root_cause_major_component_failure_non-warranty      4\n",
       "root_cause_design/sale_issue                         2\n",
       "Name: root_cause, dtype: int64"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_origin_omnidian_customer.root_cause.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_origin_homeowner = df101_a.loc[df101_a.ticket_origin =='origin_homeowner']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "root_cause_normal_wear_and_tear                    72\n",
       "root_cause_major_component_failure_warranty        19\n",
       "root_cause_non-service_support                     13\n",
       "root_cause_roof_issue                              11\n",
       "root_cause_homeowner                                9\n",
       "root_cause_installer_workmanship                    6\n",
       "root_cause_service_workmanship                      3\n",
       "root_cause_major_component_failure_non-warranty     2\n",
       "root_cause_environmental                            2\n",
       "Name: root_cause, dtype: int64"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_origin_homeowner.root_cause.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train-test-split for cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df101_a.drop('root_cause', axis=1).copy()\n",
    "y = df101_a['root_cause']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                                    random_state=42, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's try ColumnTransformer of categoricals for vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mInit signature:\u001b[0m\n",
       "\u001b[0mColumnTransformer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mtransformers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mremainder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'drop'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0msparse_threshold\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mtransformer_weights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m     \n",
       "Applies transformers to columns of an array or pandas DataFrame.\n",
       "\n",
       "This estimator allows different columns or column subsets of the input\n",
       "to be transformed separately and the features generated by each transformer\n",
       "will be concatenated to form a single feature space.\n",
       "This is useful for heterogeneous or columnar data, to combine several\n",
       "feature extraction mechanisms or transformations into a single transformer.\n",
       "\n",
       "Read more in the :ref:`User Guide <column_transformer>`.\n",
       "\n",
       ".. versionadded:: 0.20\n",
       "\n",
       "Parameters\n",
       "----------\n",
       "transformers : list of tuples\n",
       "    List of (name, transformer, column(s)) tuples specifying the\n",
       "    transformer objects to be applied to subsets of the data.\n",
       "\n",
       "    name : string\n",
       "        Like in Pipeline and FeatureUnion, this allows the transformer and\n",
       "        its parameters to be set using ``set_params`` and searched in grid\n",
       "        search.\n",
       "    transformer : estimator or {'passthrough', 'drop'}\n",
       "        Estimator must support `fit` and `transform`. Special-cased\n",
       "        strings 'drop' and 'passthrough' are accepted as well, to\n",
       "        indicate to drop the columns or to pass them through untransformed,\n",
       "        respectively.\n",
       "    column(s) : string or int, array-like of string or int, slice, boolean mask array or callable\n",
       "        Indexes the data on its second axis. Integers are interpreted as\n",
       "        positional columns, while strings can reference DataFrame columns\n",
       "        by name.  A scalar string or int should be used where\n",
       "        ``transformer`` expects X to be a 1d array-like (vector),\n",
       "        otherwise a 2d array will be passed to the transformer.\n",
       "        A callable is passed the input data `X` and can return any of the\n",
       "        above.\n",
       "\n",
       "remainder : {'drop', 'passthrough'} or estimator, default 'drop'\n",
       "    By default, only the specified columns in `transformers` are\n",
       "    transformed and combined in the output, and the non-specified\n",
       "    columns are dropped. (default of ``'drop'``).\n",
       "    By specifying ``remainder='passthrough'``, all remaining columns that\n",
       "    were not specified in `transformers` will be automatically passed\n",
       "    through. This subset of columns is concatenated with the output of\n",
       "    the transformers.\n",
       "    By setting ``remainder`` to be an estimator, the remaining\n",
       "    non-specified columns will use the ``remainder`` estimator. The\n",
       "    estimator must support :term:`fit` and :term:`transform`.\n",
       "\n",
       "sparse_threshold : float, default = 0.3\n",
       "    If the output of the different transformers contains sparse matrices,\n",
       "    these will be stacked as a sparse matrix if the overall density is\n",
       "    lower than this value. Use ``sparse_threshold=0`` to always return\n",
       "    dense.  When the transformed output consists of all dense data, the\n",
       "    stacked result will be dense, and this keyword will be ignored.\n",
       "\n",
       "n_jobs : int or None, optional (default=None)\n",
       "    Number of jobs to run in parallel.\n",
       "    ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n",
       "    ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n",
       "    for more details.\n",
       "\n",
       "transformer_weights : dict, optional\n",
       "    Multiplicative weights for features per transformer. The output of the\n",
       "    transformer is multiplied by these weights. Keys are transformer names,\n",
       "    values the weights.\n",
       "\n",
       "verbose : boolean, optional(default=False)\n",
       "    If True, the time elapsed while fitting each transformer will be\n",
       "    printed as it is completed.\n",
       "\n",
       "Attributes\n",
       "----------\n",
       "transformers_ : list\n",
       "    The collection of fitted transformers as tuples of\n",
       "    (name, fitted_transformer, column). `fitted_transformer` can be an\n",
       "    estimator, 'drop', or 'passthrough'. In case there were no columns\n",
       "    selected, this will be the unfitted transformer.\n",
       "    If there are remaining columns, the final element is a tuple of the\n",
       "    form:\n",
       "    ('remainder', transformer, remaining_columns) corresponding to the\n",
       "    ``remainder`` parameter. If there are remaining columns, then\n",
       "    ``len(transformers_)==len(transformers)+1``, otherwise\n",
       "    ``len(transformers_)==len(transformers)``.\n",
       "\n",
       "named_transformers_ : Bunch object, a dictionary with attribute access\n",
       "    Read-only attribute to access any transformer by given name.\n",
       "    Keys are transformer names and values are the fitted transformer\n",
       "    objects.\n",
       "\n",
       "sparse_output_ : boolean\n",
       "    Boolean flag indicating wether the output of ``transform`` is a\n",
       "    sparse matrix or a dense numpy array, which depends on the output\n",
       "    of the individual transformers and the `sparse_threshold` keyword.\n",
       "\n",
       "Notes\n",
       "-----\n",
       "The order of the columns in the transformed feature matrix follows the\n",
       "order of how the columns are specified in the `transformers` list.\n",
       "Columns of the original feature matrix that are not specified are\n",
       "dropped from the resulting transformed feature matrix, unless specified\n",
       "in the `passthrough` keyword. Those columns specified with `passthrough`\n",
       "are added at the right to the output of the transformers.\n",
       "\n",
       "See also\n",
       "--------\n",
       "sklearn.compose.make_column_transformer : convenience function for\n",
       "    combining the outputs of multiple transformer objects applied to\n",
       "    column subsets of the original feature space.\n",
       "\n",
       "Examples\n",
       "--------\n",
       ">>> import numpy as np\n",
       ">>> from sklearn.compose import ColumnTransformer\n",
       ">>> from sklearn.preprocessing import Normalizer\n",
       ">>> ct = ColumnTransformer(\n",
       "...     [(\"norm1\", Normalizer(norm='l1'), [0, 1]),\n",
       "...      (\"norm2\", Normalizer(norm='l1'), slice(2, 4))])\n",
       ">>> X = np.array([[0., 1., 2., 2.],\n",
       "...               [1., 1., 0., 1.]])\n",
       ">>> # Normalizer scales each row of X to unit norm. A separate scaling\n",
       ">>> # is applied for the two first and two last elements of each\n",
       ">>> # row independently.\n",
       ">>> ct.fit_transform(X)    # doctest: +NORMALIZE_WHITESPACE\n",
       "array([[0. , 1. , 0.5, 0.5],\n",
       "       [0.5, 0.5, 0. , 1. ]])\n",
       "\u001b[0;31mFile:\u001b[0m           ~/anaconda3/envs/learn2env/lib/python3.7/site-packages/sklearn/compose/_column_transformer.py\n",
       "\u001b[0;31mType:\u001b[0m           ABCMeta\n",
       "\u001b[0;31mSubclasses:\u001b[0m     \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ColumnTransformer?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "categoricals = X.select_dtypes(include=['object']).columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['ticket_id', 'asset_id', 'ticket_creation_reason', 'ticket_origin',\n",
       "       'service_partner', 'installed_by'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categoricals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_transformer = Pipeline(steps=[('onehot', OneHotEncoder(handle_unknown='ignore'))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[('cat', categorical_transformer, categoricals)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/humanperson/anaconda3/envs/learn2env/lib/python3.7/site-packages/numpy/lib/arraysetops.py:565: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  mask &= (ar1 != a)\n",
      "/Users/humanperson/anaconda3/envs/learn2env/lib/python3.7/site-packages/numpy/lib/arraysetops.py:569: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  mask |= (ar1 == a)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: 'origin_homeowner'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-49-504ac36c643e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mencoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOneHotEncoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle_unknown\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'ignore'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mX_train_enc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcategoricals\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m X_train_enc = pd.DataFrame(encoder.transform(X_train).toarray(),\n\u001b[0m\u001b[1;32m      4\u001b[0m                          columns=encoder.get_feature_names())\n\u001b[1;32m      5\u001b[0m X_test_enc = pd.DataFrame(encoder.transform(X_test).toarray(),\n",
      "\u001b[0;32m~/anaconda3/envs/learn2env/lib/python3.7/site-packages/sklearn/preprocessing/_encoders.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    730\u001b[0m                                        copy=True)\n\u001b[1;32m    731\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 732\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transform_new\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    733\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    734\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minverse_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/learn2env/lib/python3.7/site-packages/sklearn/preprocessing/_encoders.py\u001b[0m in \u001b[0;36m_transform_new\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    678\u001b[0m         \u001b[0;34m\"\"\"New implementation assuming categorical input\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    679\u001b[0m         \u001b[0;31m# validation of X happens in _check_X called by _transform\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 680\u001b[0;31m         \u001b[0mX_int\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle_unknown\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle_unknown\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    681\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    682\u001b[0m         \u001b[0mn_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_int\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/learn2env/lib/python3.7/site-packages/sklearn/preprocessing/_encoders.py\u001b[0m in \u001b[0;36m_transform\u001b[0;34m(self, X, handle_unknown)\u001b[0m\n\u001b[1;32m    134\u001b[0m                         \u001b[0mXi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mXi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 136\u001b[0;31m                     \u001b[0mXi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m~\u001b[0m\u001b[0mvalid_mask\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcategories_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    137\u001b[0m             \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_encode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcategories_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m             \u001b[0mX_int\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoded\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: could not convert string to float: 'origin_homeowner'"
     ]
    }
   ],
   "source": [
    "encoder = OneHotEncoder(handle_unknown='ignore')\n",
    "X_train_enc = encoder.fit(X_train[categoricals])\n",
    "X_train_enc = pd.DataFrame(encoder.transform(X_train).toarray(),\n",
    "                         columns=encoder.get_feature_names())\n",
    "X_test_enc = pd.DataFrame(encoder.transform(X_test).toarray(),\n",
    "                        columns=encoder.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn2env",
   "language": "python",
   "name": "learn2env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
