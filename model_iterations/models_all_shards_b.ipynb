{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In which we create vectors of all categorical features before splitting for testing, and used 'days ago' instead of datetimes so we can run more models. The results were not great because we do not have a one-to-one relationship between ticket_id and asset_id."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Factors from SQL query: 'ticket_id', 'asset_id', 'root_cause', 'ticket_creation_reason',\n",
    "####      'latitude', 'azimuth', 'ticket_origin',\n",
    "#### 'service_partner', 'ticket_assigned_days_ago', 'ticket_closed_days_ago',\n",
    "####       'installed_by', 'installed_days_ago'\n",
    "#### Models compared: 'Logistic Regression', 'Nearest Neighbors', 'RBF SVM',\n",
    "####         'Decision Tree', 'Random Forest', 'Neural Net', 'Bagging', 'AdaBoost',\n",
    "####       'Gradient Boost'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier, BaggingClassifier, AdaBoostClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from catboost import Pool, CatBoostClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from scipy import stats\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics import classification_report, log_loss, f1_score, auc\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.compose import ColumnTransformer, make_column_transformer\n",
    "from sklearn import tree\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.utils import resample\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn-paper')\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_shards_b = pd.read_csv('../data/all_shards_b.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5247 entries, 0 to 5246\n",
      "Data columns (total 11 columns):\n",
      "ticket_id                         3633 non-null float64\n",
      "asset_id                          5247 non-null object\n",
      "root_cause                        3633 non-null object\n",
      "ticket_creation_reason            3616 non-null object\n",
      "ticket_origin                     3632 non-null object\n",
      "service_partner                   3633 non-null object\n",
      "date_ticket_initially_assigned    3628 non-null object\n",
      "latitude                          3633 non-null float64\n",
      "longitude                         3633 non-null float64\n",
      "installed_by                      3615 non-null object\n",
      "installation_date                 1871 non-null object\n",
      "dtypes: float64(3), object(8)\n",
      "memory usage: 451.0+ KB\n"
     ]
    }
   ],
   "source": [
    "df_all_shards_b.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_all_shards_b.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both ticket_id and asset_id need to be strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5247 entries, 0 to 5246\n",
      "Data columns (total 11 columns):\n",
      "ticket_id                         3633 non-null object\n",
      "asset_id                          5247 non-null object\n",
      "root_cause                        3633 non-null object\n",
      "ticket_creation_reason            3616 non-null object\n",
      "ticket_origin                     3632 non-null object\n",
      "service_partner                   3633 non-null object\n",
      "date_ticket_initially_assigned    3628 non-null object\n",
      "latitude                          3633 non-null float64\n",
      "longitude                         3633 non-null float64\n",
      "installed_by                      3615 non-null object\n",
      "installation_date                 1871 non-null object\n",
      "dtypes: float64(2), object(9)\n",
      "memory usage: 451.0+ KB\n"
     ]
    }
   ],
   "source": [
    "df_all_shards_b[['ticket_id', 'asset_id']] = df_all_shards_b[['ticket_id', 'asset_id']].astype(object)\n",
    "df_all_shards_b.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3591"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all_shards_b.ticket_id.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deal with duplicates and nulls. A lot of this data came from the time before Omnidian. It's missing information we consider relevant, so we'll drop those columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 4193 entries, 0 to 5246\n",
      "Data columns (total 11 columns):\n",
      "ticket_id                         3591 non-null object\n",
      "asset_id                          4193 non-null object\n",
      "root_cause                        3591 non-null object\n",
      "ticket_creation_reason            3574 non-null object\n",
      "ticket_origin                     3590 non-null object\n",
      "service_partner                   3591 non-null object\n",
      "date_ticket_initially_assigned    3586 non-null object\n",
      "latitude                          3591 non-null float64\n",
      "longitude                         3591 non-null float64\n",
      "installed_by                      3573 non-null object\n",
      "installation_date                 1829 non-null object\n",
      "dtypes: float64(2), object(9)\n",
      "memory usage: 393.1+ KB\n"
     ]
    }
   ],
   "source": [
    "df_all_shards_b.drop_duplicates(inplace=True)\n",
    "df_all_shards_b.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ticket_id                          True\n",
       "asset_id                          False\n",
       "root_cause                         True\n",
       "ticket_creation_reason             True\n",
       "ticket_origin                      True\n",
       "service_partner                    True\n",
       "date_ticket_initially_assigned     True\n",
       "latitude                           True\n",
       "longitude                          True\n",
       "installed_by                       True\n",
       "installation_date                  True\n",
       "dtype: bool"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all_shards_b.isnull().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 1792 entries, 0 to 5085\n",
      "Data columns (total 11 columns):\n",
      "ticket_id                         1792 non-null object\n",
      "asset_id                          1792 non-null object\n",
      "root_cause                        1792 non-null object\n",
      "ticket_creation_reason            1792 non-null object\n",
      "ticket_origin                     1792 non-null object\n",
      "service_partner                   1792 non-null object\n",
      "date_ticket_initially_assigned    1792 non-null object\n",
      "latitude                          1792 non-null float64\n",
      "longitude                         1792 non-null float64\n",
      "installed_by                      1792 non-null object\n",
      "installation_date                 1792 non-null object\n",
      "dtypes: float64(2), object(9)\n",
      "memory usage: 168.0+ KB\n"
     ]
    }
   ],
   "source": [
    "df_all_shards_b.dropna(axis=0, how='any', inplace=True)\n",
    "df_all_shards_b.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert everthing to numbers for our machine to read."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We want to use ticket_id to look things up later and will not encode it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 1792 entries, 0 to 5085\n",
      "Data columns (total 9 columns):\n",
      "asset_id                          1792 non-null object\n",
      "ticket_creation_reason            1792 non-null object\n",
      "ticket_origin                     1792 non-null object\n",
      "service_partner                   1792 non-null object\n",
      "date_ticket_initially_assigned    1792 non-null object\n",
      "latitude                          1792 non-null float64\n",
      "longitude                         1792 non-null float64\n",
      "installed_by                      1792 non-null object\n",
      "installation_date                 1792 non-null object\n",
      "dtypes: float64(2), object(7)\n",
      "memory usage: 140.0+ KB\n"
     ]
    }
   ],
   "source": [
    "df_sans_ticket = df_all_shards_b.drop(['ticket_id', 'root_cause'], axis=1).copy()\n",
    "df_sans_ticket.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['asset_id',\n",
       " 'ticket_creation_reason',\n",
       " 'ticket_origin',\n",
       " 'service_partner',\n",
       " 'date_ticket_initially_assigned',\n",
       " 'installed_by',\n",
       " 'installation_date']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# List our categorical features\n",
    "categoricals = list(df_sans_ticket.columns[(df_sans_ticket.dtypes.values == np.dtype('object'))])\n",
    "categoricals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OneHotEncoder(categorical_features=None, categories='auto', drop=None,\n",
       "              dtype=<class 'numpy.float64'>, handle_unknown='ignore',\n",
       "              n_values=None, sparse=True)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder = OneHotEncoder(categories='auto', handle_unknown='ignore')\n",
    "encoder.fit(df_sans_ticket[categoricals])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1792 entries, 0 to 1791\n",
      "Columns: 3962 entries, x0_101111473 to x6_2016-09-06 00:00:00\n",
      "dtypes: float64(3962)\n",
      "memory usage: 54.2 MB\n"
     ]
    }
   ],
   "source": [
    "enc_cat = pd.DataFrame(encoder.transform(df_sans_ticket[categoricals]).toarray(),\n",
    "                         columns=encoder.get_feature_names())\n",
    "enc_cat.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x0_101111473</th>\n",
       "      <th>x0_101111478</th>\n",
       "      <th>x0_101111485</th>\n",
       "      <th>x0_101111489</th>\n",
       "      <th>x0_101111497</th>\n",
       "      <th>x0_101111499</th>\n",
       "      <th>x0_101111511</th>\n",
       "      <th>x0_101111513</th>\n",
       "      <th>x0_101111521</th>\n",
       "      <th>x0_101111528</th>\n",
       "      <th>...</th>\n",
       "      <th>x6_2016-04-19 00:00:00</th>\n",
       "      <th>x6_2016-04-27 00:00:00</th>\n",
       "      <th>x6_2016-05-11 00:00:00</th>\n",
       "      <th>x6_2016-05-18 00:00:00</th>\n",
       "      <th>x6_2016-06-29 00:00:00</th>\n",
       "      <th>x6_2016-07-01 00:00:00</th>\n",
       "      <th>x6_2016-07-09 00:00:00</th>\n",
       "      <th>x6_2016-08-03 00:00:00</th>\n",
       "      <th>x6_2016-08-09 00:00:00</th>\n",
       "      <th>x6_2016-09-06 00:00:00</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 3962 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   x0_101111473  x0_101111478  x0_101111485  x0_101111489  x0_101111497  \\\n",
       "0           0.0           0.0           0.0           0.0           0.0   \n",
       "1           0.0           0.0           0.0           0.0           0.0   \n",
       "2           0.0           0.0           0.0           0.0           0.0   \n",
       "3           0.0           0.0           0.0           0.0           0.0   \n",
       "4           0.0           0.0           0.0           0.0           0.0   \n",
       "\n",
       "   x0_101111499  x0_101111511  x0_101111513  x0_101111521  x0_101111528  ...  \\\n",
       "0           0.0           0.0           0.0           0.0           0.0  ...   \n",
       "1           0.0           0.0           0.0           0.0           0.0  ...   \n",
       "2           0.0           0.0           0.0           0.0           0.0  ...   \n",
       "3           0.0           0.0           0.0           0.0           0.0  ...   \n",
       "4           0.0           0.0           0.0           0.0           0.0  ...   \n",
       "\n",
       "   x6_2016-04-19 00:00:00  x6_2016-04-27 00:00:00  x6_2016-05-11 00:00:00  \\\n",
       "0                     0.0                     0.0                     0.0   \n",
       "1                     0.0                     0.0                     0.0   \n",
       "2                     0.0                     0.0                     0.0   \n",
       "3                     0.0                     0.0                     0.0   \n",
       "4                     0.0                     0.0                     0.0   \n",
       "\n",
       "   x6_2016-05-18 00:00:00  x6_2016-06-29 00:00:00  x6_2016-07-01 00:00:00  \\\n",
       "0                     0.0                     0.0                     0.0   \n",
       "1                     0.0                     0.0                     0.0   \n",
       "2                     0.0                     0.0                     0.0   \n",
       "3                     0.0                     0.0                     0.0   \n",
       "4                     0.0                     0.0                     0.0   \n",
       "\n",
       "   x6_2016-07-09 00:00:00  x6_2016-08-03 00:00:00  x6_2016-08-09 00:00:00  \\\n",
       "0                     0.0                     0.0                     0.0   \n",
       "1                     0.0                     0.0                     0.0   \n",
       "2                     0.0                     0.0                     0.0   \n",
       "3                     0.0                     0.0                     0.0   \n",
       "4                     0.0                     0.0                     0.0   \n",
       "\n",
       "   x6_2016-09-06 00:00:00  \n",
       "0                     0.0  \n",
       "1                     0.0  \n",
       "2                     0.0  \n",
       "3                     0.0  \n",
       "4                     0.0  \n",
       "\n",
       "[5 rows x 3962 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# enc_cat.insert(loc=0, column='ticket_id', value=df101_e.ticket_id)\n",
    "enc_cat.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 1792 entries, 0 to 5085\n",
      "Data columns (total 4 columns):\n",
      "ticket_id     1792 non-null object\n",
      "root_cause    1792 non-null object\n",
      "latitude      1792 non-null float64\n",
      "longitude     1792 non-null float64\n",
      "dtypes: float64(2), object(2)\n",
      "memory usage: 70.0+ KB\n"
     ]
    }
   ],
   "source": [
    "# grab the other features\n",
    "df_other = df_all_shards_b.drop(categoricals, axis=1).copy()\n",
    "df_other.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 1792 entries, 0 to 5085\n",
      "Columns: 3966 entries, ticket_id to x6_2016-09-06 00:00:00\n",
      "dtypes: float64(3964), object(2)\n",
      "memory usage: 54.3+ MB\n"
     ]
    }
   ],
   "source": [
    "# put it back together\n",
    "df_enc = df_other.join(enc_cat)\n",
    "df_enc.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3962"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_enc.isna().any().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ticket_id                 False\n",
       "root_cause                False\n",
       "latitude                  False\n",
       "longitude                 False\n",
       "x0_101111473               True\n",
       "x0_101111478               True\n",
       "x0_101111485               True\n",
       "x0_101111489               True\n",
       "x0_101111497               True\n",
       "x0_101111499               True\n",
       "x0_101111511               True\n",
       "x0_101111513               True\n",
       "x0_101111521               True\n",
       "x0_101111528               True\n",
       "x0_101111533               True\n",
       "x0_101111565               True\n",
       "x0_101111567               True\n",
       "x0_101111573               True\n",
       "x0_101111576               True\n",
       "x0_101111589               True\n",
       "x0_101111606               True\n",
       "x0_101111614               True\n",
       "x0_101111621               True\n",
       "x0_101111625               True\n",
       "x0_101111629               True\n",
       "x0_101111640               True\n",
       "x0_101111652               True\n",
       "x0_101111662               True\n",
       "x0_101111673               True\n",
       "x0_101111679               True\n",
       "                          ...  \n",
       "x6_2015-12-28 00:00:00     True\n",
       "x6_2015-12-29 00:00:00     True\n",
       "x6_2016-01-04 00:00:00     True\n",
       "x6_2016-01-06 00:00:00     True\n",
       "x6_2016-01-07 00:00:00     True\n",
       "x6_2016-01-11 00:00:00     True\n",
       "x6_2016-01-14 00:00:00     True\n",
       "x6_2016-01-20 00:00:00     True\n",
       "x6_2016-02-04 00:00:00     True\n",
       "x6_2016-02-15 00:00:00     True\n",
       "x6_2016-02-19 00:00:00     True\n",
       "x6_2016-02-25 00:00:00     True\n",
       "x6_2016-02-26 00:00:00     True\n",
       "x6_2016-03-09 00:00:00     True\n",
       "x6_2016-03-16 00:00:00     True\n",
       "x6_2016-03-17 00:00:00     True\n",
       "x6_2016-03-21 00:00:00     True\n",
       "x6_2016-03-22 00:00:00     True\n",
       "x6_2016-03-31 00:00:00     True\n",
       "x6_2016-04-15 00:00:00     True\n",
       "x6_2016-04-19 00:00:00     True\n",
       "x6_2016-04-27 00:00:00     True\n",
       "x6_2016-05-11 00:00:00     True\n",
       "x6_2016-05-18 00:00:00     True\n",
       "x6_2016-06-29 00:00:00     True\n",
       "x6_2016-07-01 00:00:00     True\n",
       "x6_2016-07-09 00:00:00     True\n",
       "x6_2016-08-03 00:00:00     True\n",
       "x6_2016-08-09 00:00:00     True\n",
       "x6_2016-09-06 00:00:00     True\n",
       "Length: 3966, dtype: bool"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_enc.isnull().any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### It looks like the 0.0 from our binary encoding turned into nulls. We'll change them back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_enc.fillna(0.0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_enc.isna().any().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train-test-split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_enc = df_enc.drop(['root_cause'], axis=1).copy()\n",
    "y_enc = df_enc['root_cause']\n",
    "X_train_enc, X_test_enc, y_train_enc, y_test_enc = train_test_split(X_enc, y_enc, random_state=42,\n",
    "                                                                    test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train_enc.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 1433 entries, 1172 to 1187\n",
      "Columns: 3965 entries, ticket_id to x6_2016-09-06 00:00:00\n",
      "dtypes: float64(3965)\n",
      "memory usage: 43.4 MB\n"
     ]
    }
   ],
   "source": [
    "X_train_enc.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1433,)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_enc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_test_enc.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 359 entries, 685 to 1380\n",
      "Columns: 3965 entries, ticket_id to x6_2016-09-06 00:00:00\n",
      "dtypes: float64(3965)\n",
      "memory usage: 10.9 MB\n"
     ]
    }
   ],
   "source": [
    "X_test_enc.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(359,)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_enc.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_state=42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = ['Logistic Regression', 'Nearest Neighbors', 'RBF SVM',\n",
    "         'Decision Tree', 'Random Forest', 'Neural Net', 'Bagging', 'AdaBoost',\n",
    "         'Gradient Boost']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/humanperson/anaconda3/envs/learn2env/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
       "                   multi_class='multinomial', n_jobs=None, penalty='l2',\n",
       "                   random_state=42, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = LogisticRegression(random_state=42, solver='lbfgs',  multi_class='multinomial', max_iter=1000)\n",
    "lr.fit(X_train_enc, y_train_enc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Let's pickle this model\n",
    "filename = '../pickled_models/shards_lr.pkl'\n",
    "pickle.dump(lr, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4735376044568245"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.score(X_test_enc, y_test_enc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "f1_score() missing 2 required positional arguments: 'y_true' and 'y_pred'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-1bfa6f8688c0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mf1_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: f1_score() missing 2 required positional arguments: 'y_true' and 'y_pred'"
     ]
    }
   ],
   "source": [
    "f1_score()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_proba1 = lr.predict_proba(X_test_enc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(359, 10)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_proba1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/humanperson/anaconda3/envs/learn2env/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight='balanced', dual=False,\n",
       "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
       "                   max_iter=200, multi_class='multinomial', n_jobs=None,\n",
       "                   penalty='l2', random_state=42, solver='lbfgs', tol=0.0001,\n",
       "                   verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#try weighing less common classes. This is not great right now because the test data may not have all classes.\n",
    "lr_w = LogisticRegression(random_state=42, class_weight='balanced', solver='lbfgs',\n",
    "                        multi_class='multinomial', max_iter=200)\n",
    "lr_w.fit(X_train_enc, y_train_enc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.05013927576601671"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_w.score(X_test_enc, y_test_enc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's pickle this model\n",
    "filename = '../pickled_models/shards_lr_w.pkl'\n",
    "pickle.dump(lr_w, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lr.score(X_test_enc, y_test_enc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4206128133704735"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn = KNeighborsClassifier()\n",
    "knn.fit(X_train_enc, y_train_enc)\n",
    "knn.score(X_test_enc, y_test_enc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's pickle this model\n",
    "filename = '../pickled_models/shards_knn.pkl'\n",
    "pickle.dump(knn, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4735376044568245"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svc_g = SVC(gamma=2, C=1)\n",
    "svc_g.fit(X_train_enc, y_train_enc)\n",
    "svc_g.score(X_test_enc, y_test_enc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's pickle this model\n",
    "filename = '../pickled_models/shards_svc_g.pkl'\n",
    "pickle.dump(svc_g, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4456824512534819"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt = DecisionTreeClassifier(min_samples_leaf=30)\n",
    "dt.fit(X_train_enc, y_train_enc)\n",
    "dt.score(X_test_enc, y_test_enc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's pickle this model\n",
    "filename = '../pickled_models/shards_dt.pkl'\n",
    "pickle.dump(dt, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4623955431754875"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_entropy = RandomForestClassifier(n_estimators=100,criterion='entropy')\n",
    "rf_entropy.fit(X_train_enc, y_train_enc)\n",
    "rf_entropy.score(X_test_enc, y_test_enc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's pickle this model\n",
    "filename = '../pickled_models/shards_rf_entropy.pkl'\n",
    "pickle.dump(rf_entropy, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.46518105849582175"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_gini = RandomForestClassifier(n_estimators=100,criterion='gini')\n",
    "rf_gini.fit(X_train_enc, y_train_enc)\n",
    "rf_gini.score(X_test_enc, y_test_enc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's pickle this model\n",
    "filename = '../pickled_models/shards_rf_gini.pkl'\n",
    "pickle.dump(rf_gini, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.14484679665738162"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp = MLPClassifier(max_iter=100)\n",
    "mlp.fit(X_train_enc, y_train_enc)\n",
    "mlp.score(X_test_enc, y_test_enc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's pickle this model\n",
    "filename = '../pickled_models/mlp.pkl'\n",
    "pickle.dump(mlp, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4735376044568245"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag = BaggingClassifier(random_state=42, bootstrap_features=True)\n",
    "bag.fit(X_train_enc, y_train_enc)\n",
    "bag.score(X_test_enc, y_test_enc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's pickle this model\n",
    "filename = '../pickled_models/shards_bag.pkl'\n",
    "pickle.dump(bag, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.47075208913649025"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ada = AdaBoostClassifier(random_state=42)\n",
    "ada.fit(X_train_enc, y_train_enc)\n",
    "ada.score(X_test_enc, y_test_enc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's pickle this model\n",
    "filename = '../pickled_models/shards_ada.pkl'\n",
    "pickle.dump(ada, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4568245125348189"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gb = GradientBoostingClassifier(random_state=42, min_samples_leaf=30)\n",
    "gb.fit(X_train_enc, y_train_enc)\n",
    "gb.score(X_test_enc, y_test_enc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's pickle this model\n",
    "filename = '../pickled_models/shards_gb.pkl'\n",
    "pickle.dump(gb, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: -2.1063086\ttotal: 65.1ms\tremaining: 585ms\n",
      "1:\tlearn: -1.9087747\ttotal: 70.2ms\tremaining: 281ms\n",
      "2:\tlearn: -1.7167418\ttotal: 74.5ms\tremaining: 174ms\n",
      "3:\tlearn: -1.6997964\ttotal: 78.4ms\tremaining: 118ms\n",
      "4:\tlearn: -1.6646526\ttotal: 82.5ms\tremaining: 82.5ms\n",
      "5:\tlearn: -1.6623159\ttotal: 86.6ms\tremaining: 57.8ms\n",
      "6:\tlearn: -1.6543571\ttotal: 90.8ms\tremaining: 38.9ms\n",
      "7:\tlearn: -1.6198531\ttotal: 95.1ms\tremaining: 23.8ms\n",
      "8:\tlearn: -1.6173163\ttotal: 99.3ms\tremaining: 11ms\n",
      "9:\tlearn: -1.6024950\ttotal: 104ms\tremaining: 0us\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<catboost.core.CatBoostClassifier at 0x1343d50b8>"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat = CatBoostClassifier(iterations=10,\n",
    "                           learning_rate=1,\n",
    "                           depth=2,\n",
    "                           loss_function='MultiClass')\n",
    "cat.fit(X_train_enc, y_train_enc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'learn': {'MultiClass': -1.602495019767117}}"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = '../pickled_models/shards_cat.pkl'\n",
    "pickle.dump(cat, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn2env",
   "language": "python",
   "name": "learn2env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
